<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Human-Centred AI — Essay — Nirav Ramidi</title>
  <meta name="description" content="A synthesis on Human-Centred AI: principles, design patterns, trade-offs, evaluation, and governance. Focus on building trustworthy, useful, and fair AI systems." />

  <!-- Styles (kept separate) -->
  <link rel="stylesheet" href="../assets/css/main.css" />
  <link rel="icon" href="../assets/img/favicon.png" />

  <!-- Structured data -->
  <script type="application/ld+json">
  {
    "@context":"https://schema.org",
    "@type":"ScholarlyArticle",
    "headline":"Human-Centred AI — Essay",
    "author":{"@type":"Person","name":"Nirav Ramidi"},
    "about":"Human-Centred AI, explainability, usability, trust, fairness, governance",
    "inLanguage":"en"
  }
  </script>
</head>

<body>
  <a class="skip-link" href="#content">Skip to content</a>

  <!-- Header / Nav -->
  <header class="site-header" role="banner">
    <div class="container header-inner">
      <a class="brand" href="../" aria-label="Nirav Ramidi — home">
        <span class="brand__mark" aria-hidden="true">NR</span>
        <span class="brand__text">Nirav Ramidi</span>
      </a>

      <nav class="site-nav" aria-label="Primary">
        <ul class="nav-list">
          <li><a href="../">Home</a></li>
          <li><a href="../experience.html">Experience</a></li>
          <li><a href="../projects/">Projects</a></li>
          <li><a href="./" aria-current="page">Writing</a></li>
          <li><a href="../education/">Education</a></li>
          <li><a href="../about.html">About</a></li>
          <li class="hide-on-mobile"><a href="../resume/" class="btn btn--ghost">Resume</a></li>
          <li class="nav-icons">
            <a href="https://github.com/Nirav01" target="_blank" rel="noopener" aria-label="GitHub">GitHub</a>
            <a href="https://www.linkedin.com/in/nirav-ramidi/" target="_blank" rel="noopener" aria-label="LinkedIn">LinkedIn</a>
          </li>
        </ul>
      </nav>

      <a href="../resume/Nirav_Ramidi_CV.pdf" class="btn btn--primary show-on-mobile">Download CV</a>
    </div>
  </header>

  <main id="content" class="site-main" role="main">
    <!-- Breadcrumbs -->
    <nav class="breadcrumbs container" aria-label="Breadcrumb">
      <ol>
        <li><a href="../">Home</a></li>
        <li><a href="./">Writing</a></li>
        <li><span aria-current="page">Human-Centred AI — Essay</span></li>
      </ol>
    </nav>

    <!-- Hero -->
    <section class="case-hero section">
      <div class="container case-hero__inner">
        <div class="case-hero__copy">
          <h1>Human-Centred AI — Essay</h1>
          <p class="lead">
            A synthesis on designing AI systems that are <strong>useful</strong>, <strong>trustworthy</strong>, and
            <strong>fair</strong>. I discuss core principles, pragmatic design patterns, trade-offs, evaluation
            strategies, and lightweight governance for teams shipping ML features.
          </p>
          <ul class="metrics">
            <li class="chip">Explainability</li>
            <li class="chip">Usability &amp; Trust</li>
            <li class="chip">Fairness</li>
            <li class="chip">Evaluation &amp; Ops</li>
          </ul>
          <div class="case-hero__buttons">
            <!-- Place your PDF at /writing/HCAI-essay.pdf to activate -->
            <a class="btn btn--primary" href="HCAI-essay.pdf">Download full PDF</a>
            <a class="btn btn--ghost" href="../contact.html">Discuss this essay</a>
          </div>
        </div>
        <figure class="case-hero__media">
          <img src="../assets/img/writing/hcai-essay-hero.jpg" alt="Human-centred AI diagram with people, model, and feedback loops" loading="lazy" />
        </figure>
      </div>
    </section>

    <!-- Layout -->
    <section class="case-layout section">
      <div class="container grid case-grid">
        <!-- Meta -->
        <aside class="case-meta card">
          <h2 class="visually-hidden">Article info</h2>
          <dl class="meta-list">
            <dt>Type</dt><dd>Essay / literature synthesis</dd>
            <dt>Focus</dt><dd>Design principles · Trade-offs · Evaluation · Governance</dd>
            <dt>Artifacts</dt><dd>PDF essay, slide summary</dd>
            <dt>Keywords</dt><dd>HCAI, XAI, Trust, Fairness, UX, Safety</dd>
          </dl>
          <div class="note small">
            <p><strong>Tip:</strong> Put <code>HCAI-essay.pdf</code> in this folder to enable the download link.</p>
          </div>
        </aside>

        <!-- Body -->
        <article class="case-body">
          <!-- Thesis -->
          <section class="case-section" id="thesis">
            <h2>Thesis</h2>
            <p>
              Human-Centred AI (HCAI) reframes the goal of AI from “maximising model accuracy” to
              “<em>maximising human outcomes</em>.” Usability, interpretability, and fairness are not
              “add-ons” but <strong>design constraints</strong> that shape the product surface and the
              technical stack. An HCAI system makes <em>appropriate use</em> of automation: it is
              legible, contestable, and fails safely.
            </p>
          </section>

          <!-- Principles -->
          <section class="case-section" id="principles">
            <h2>Principles</h2>
            <ul class="checklist">
              <li><strong>Clarity over completeness:</strong> provide the <em>right</em> explanation for the decision at hand; defer detail via progressive disclosure.</li>
              <li><strong>Calibrated transparency:</strong> expose <em>uncertainty</em>, <em>confidence</em>, and known limitations; avoid false certainty.</li>
              <li><strong>Recourse:</strong> show users how to improve outcomes or fix inputs—not just why the model predicted something.</li>
              <li><strong>Human-in/on-the-loop:</strong> keep escalation paths and reversible actions; log overrides to learn from expert judgement.</li>
              <li><strong>Fairness by design:</strong> make sensitive features and group metrics visible during development; measure harms and benefits.</li>
              <li><strong>Privacy &amp; consent:</strong> minimise data collection; communicate how data feeds models and audits.</li>
            </ul>
          </section>

          <!-- Patterns -->
          <section class="case-section" id="patterns">
            <h2>Design Patterns</h2>
            <div class="grid cards">
              <article class="card">
                <header class="card__header"><h3>Model Cards &amp; Data Sheets</h3></header>
                <p class="card__body">A one-page summary of intended use, training data, metrics, and limits. Link from the UI for expert users.</p>
              </article>
              <article class="card">
                <header class="card__header"><h3>Confidence &amp; Uncertainty</h3></header>
                <p class="card__body">Show calibrated scores, abstain when unsure, and highlight edge cases; avoid binary “AI says so.”</p>
              </article>
              <article class="card">
                <header class="card__header"><h3>Counterfactuals &amp; Recourse</h3></header>
                <p class="card__body">“If X were Y, the outcome would change.” Provide actionable edits users can make, bounded by policy.</p>
              </article>
              <article class="card">
                <header class="card__header"><h3>Provenance &amp; Versioning</h3></header>
                <p class="card__body">Surface model/version, data cut, and time of decision; enable traceability for appeals and audits.</p>
              </article>
              <article class="card">
                <header class="card__header"><h3>Progressive Disclosure</h3></header>
                <p class="card__body">Start with a short rationale; let users expand for features, weights/shap values, and examples.</p>
              </article>
            </div>
          </section>

          <!-- Trade-offs -->
          <section class="case-section" id="tradeoffs">
            <h2>Trade-offs &amp; Pitfalls</h2>
            <ul>
              <li><strong>Clarity vs fidelity:</strong> simpler explanations aid actionability but may omit nuance; provide “learn more” for power users.</li>
              <li><strong>Performance vs interpretability:</strong> use <em>post-hoc</em> explanations (e.g., SHAP) when intrinsically simple models underfit. Validate explanations with users.</li>
              <li><strong>Over-trust vs under-trust:</strong> confidence displays and abstention thresholds reduce automation bias but may increase workload; tune with task studies.</li>
              <li><strong>Fairness trade-offs:</strong> equalising metrics for one group can reduce accuracy for another; make the choices explicit and governed.</li>
              <li><strong>Antipatterns:</strong> explanation-washing, “black-box” buttons, hidden thresholds, and dark patterns that nudge acceptance.</li>
            </ul>
          </section>

          <!-- Evaluation -->
          <section class="case-section" id="evaluation">
            <h2>Evaluation Framework</h2>
            <div class="cols">
              <div>
                <h3>Quantitative</h3>
                <ul>
                  <li><strong>Task success</strong>, <strong>time-on-task</strong>, and <strong>error rates</strong>.</li>
                  <li><strong>Trust</strong> scales, <strong>SUS</strong> (usability), <strong>NASA-TLX</strong> (workload).</li>
                  <li><strong>Calibration</strong>: Brier score / reliability curves for probabilities.</li>
                  <li><strong>Fairness</strong> metrics: TPR/FPR parity, equalised odds, demographic parity (when appropriate).</li>
                </ul>
              </div>
              <div>
                <h3>Qualitative</h3>
                <ul>
                  <li>Think-aloud protocols; semi-structured interviews.</li>
                  <li>Thematic coding of pain points and mental models.</li>
                  <li>Field observation to spot real-world workaround behaviour.</li>
                </ul>
                <h3>Study Designs</h3>
                <ul>
                  <li>A/B or within-subjects with counter-balancing.</li>
                  <li>Power analysis for sample size; pre-registered analysis plan.</li>
                </ul>
              </div>
            </div>
          </section>

          <!-- Ops & Governance -->
          <section class="case-section" id="governance">
            <h2>Operations &amp; Governance</h2>
            <ul class="checklist">
              <li><strong>Monitoring:</strong> live dashboards for quality, drift, calibration, and group fairness.</li>
              <li><strong>Feedback channels:</strong> capture overrides and user flags; use them to retrain or adjust thresholds.</li>
              <li><strong>Change control:</strong> version models and policies; run canaries and shadow tests.</li>
              <li><strong>Privacy:</strong> log minimal data; protect PII; document retention windows.</li>
              <li><strong>Documentation:</strong> model cards, decision logs, and clear appeal paths.</li>
            </ul>
          </section>

          <!-- Checklist -->
          <section class="case-section" id="checklist">
            <h2>Team Checklist</h2>
            <ol class="checklist numbered">
              <li>Define the <em>human goal</em> and the user’s decision context.</li>
              <li>Choose the <em>minimum viable explanation</em> and add deeper layers on demand.</li>
              <li>Expose <em>uncertainty</em> and handle abstention pathways.</li>
              <li>Measure <em>calibration</em> and <em>fairness</em> continuously; alert on regressions.</li>
              <li>Provide <em>recourse</em> and an <em>appeal</em> mechanism; close the loop with users.</li>
            </ol>
          </section>

          <!-- Closing -->
          <section class="case-section" id="closing">
            <h2>Conclusion</h2>
            <p>
              Human-Centred AI treats people as <em>primary stakeholders</em>, not friction in a prediction pipeline.
              When we design for clarity, recourse, and fairness—and validate with real users—AI features become
              reliable partners rather than inscrutable oracles.
            </p>
            <p class="muted small">References and extended notes are available in the PDF.</p>
          </section>

          <!-- Footer Nav -->
          <nav class="case-pager" aria-label="Writing pagination">
            <a class="btn btn--ghost" href="./">← All writing</a>
            <div class="spacer"></div>
            <a class="btn btn--secondary" href="notes/queueing-cheatsheet.html">Next: Queueing Cheatsheet →</a>
          </nav>
        </article>
      </div>
    </section>
  </main>

  <!-- Footer -->
  <footer class="site-footer" role="contentinfo">
    <div class="container footer-inner">
      <p>© <span id="year">2025</span> Nirav Ramidi</p>
      <ul class="footer-links">
        <li><a href="https://github.com/Nirav01" target="_blank" rel="noopener">GitHub</a></li>
        <li><a href="https://www.linkedin.com/in/nirav-ramidi/" target="_blank" rel="noopener">LinkedIn</a></li>
        <li><a href="../resume/">Resume</a></li>
        <li><a href="../contact.html">Contact</a></li>
      </ul>
    </div>
  </footer>

  <!-- Scripts (kept separate) -->
  <script src="../assets/js/main.js" defer></script>
</body>
</html>
